# Project Integrate

Welcome to Project Integrate. This document is a full system overview of our project, including a detailed explanation of what Integrate does, how it works and the varying technologies used. That being said, please keep in mind that this is a "living" document, and will continue to evolve as the project develops. Things you see here may change, though the underlying logic of the project and the basic technologies used will likely remain the same. 

### What Integrate Does

Project Integrate is a smart home automation system that is built to track user activity and make intelligent decisions based on that activity. In other word's, it's designed to turn on smart devices when the user is in close proximity to them, and turn them off when the user is far away from them. While simple in concept, Integrate has a few advantages over proprietary systems that we believe give it a competitive advantage. These include: 
- **Simple setup and maintanince:**
    - Integrate is designed to be simple to install and invisible while operating. The entire system is designed to be forgetten and self-maintaining, allowing users to enjoy it's benefits without worrying about keeping it running.
- **No user interaction necessary:**
    - By default, Integrate will only require users to "register" their smart home devices with the system. Users don't need to register their personal devices with the system, which means that they don't need to know anything about IP/MAC addresses or any information Integrate uses to track their devices. 
- **Privacy first:**
    - No user data leaves the system or the user's network. This keeps user data local, and thus makes it more difficult for a malicious actor to steal data.

All of these factors and more make Integrate a competitive offering in a market that has become increasingly saturated with "basic" smart home devices which offer no real difference between the smart and "unsmart" version of the products. Our system allows users to stop worrying about leaving devices on or managing automation across multiple platforms, instead allowing users to push the responsiblity of interfacing with the smart devices directly onto our system and reducing complexity at the local level. 

### How Integrate Works

In summary, Integrate uses 4 Raspberry Pi's to create a plane in which user's network activity can be tracked, then uses user's network activity to determine their location and proximity to their smart devices. Then, once they are close enough, it activates their device, thus meaning that no user interaction is necessary. 

In order to do this, we use a user's existing Wi-Fi network and protocols already used in network operations, as well as our own custom protocol, to facilitate tracking of user devices, smart devices and the Pi's themselves. 

**Here's how:**

First, a user plugs in one of the Raspberry Pi's (in our case, the [Model 3B+](https://datasheets.raspberrypi.com/rpi3/raspberry-pi-3-b-plus-product-brief.pdf)). As soon as the user plugs in the Pi, the OS immediately begins it's startup routine. The code that actually runs Integrate is already installed as a cron job that triggers automatically on system reboot, meaning that the code will begin to run as soon as the operating system is online. 

Integrate was designed from the ground up to be modular and "plug-and-play". Enforcing this design philosophy here means that users shouldn't need to know exactly which Pi should be plugged in first in order for the system to work properly. In order to create this modularity, Integrate's Pi's (henceforth referred to as "nodes") are all loaded with the exact same code binaries. Our implementation uses C++ in order to avoid the overhead that Python brings, thus allowing Integrate to run on more resource-constrained devices.  

At startup, the program first creates a log (for easier debugging of the system should anything go wrong) and creates a UDP socket that listens and sends data on a default "pairing port". In order to build the project on more modern protocols, the code utilizes IPv6 by default. The code also changes the scheduling policy of the thread handles we create for other functionalities (described later) to SCHED_RR, which allows us to control the priority of the threads that we'll create later. 

Then, it sends an LML pairing packet on the network with the default link-local address for IPv6 (for more information on the LML protocol, please look [here](LML_Protocol.md)). After sending out the initial pairing packet, the node immediately places itself in "pairing mode". This means that for a certain amount of time, the node will listen for a response on the same port. Should it receive a response, the node will first verify that the packet is formatted correctly as dictated by the LML protocol. Afterwards, it will check the packet to confirm that the existence of a certain field that is only sent by a "root" node (specifically, the `port_to_communicate` field). If so, the node immediately exits pairing mode and begins it's "leaf" thread. 

Otherwise, if the node does **not** hear from any other node in the predetermined time frame, it assumes the role of the "root" node and begins it's "root" thread. 

Regardless of which role the node assumes, the thread called after the pairing process (so either the leaf or root thread) is assigned the maximum priority Linux allows for the Round Robin scheduler, which is 99. This allows this thread to deal with the "decision making" and have priority over any threads it creates (as long as their priroity is <= 98, which is the case on our system). 

In order to make the system's operation easier to understand, from here on explanation is split between two sections: the "root" node and the "leaf" node. Each section will explain the operation of a node based on what role it occupies on the network. 

#### Root Node

##### Pairing Phase

Upon starting the "root" thread, the node immediately restarts it's pairing mode, with a few key differences. Firstly, the root will immediately begin listening on the broadcast socket (bound to the pairing port) without sending any data out to the network. This is because leaves are responsible for broadcasting their presence to the root, which means that the root shouldn't need to communicate it's presence on the network for a leaf to tell the parent it exists. 

Upon receiving a pairing packet from a leaf node, the root immediately determines several things:
1. **Port number** - the root will choose which port number will be used for communication between the root and that specific leaf node. This ensures clear communication between the root and that specific leaf node, so that messages to the root will not be confused with messages from other leaf nodes. 
2. **Type of Transport Layer Protocol** - the root chooses what transport layer protocol to use based on the amount of noise currently on the network. This allows for flexible communication methods by giving the system the ability to adapt to network conditions and, should the network be mostly interference free, keeps us from the extra overhead that TCP creates. 
3. **Interval to send data** - the root chooses how often the leaf node communicates distance measurements, thus allowing it to choose an interval that avoids unneccesary network congestion while still communicating data regularly enough to enable the root to make decisions "on the fly". 

These are communicated to the leaf node using the LML protocol, and the parent specifically communicates these to the node using it's IPv6 address (given by the pairing packet that was sent to the root) and the pairing port. An example of this response is below:
```
{
 type: pairing,
 noise: -73, 
 port_to_communicate: 27888,
 interval: 100 
}
```
<sub>Please note that noise is measured in decibals (dB), and the interval for communication is measured in milliseconds (ms).</sub>

Upon receiving this response, the child will send a confirmation packet over the **pairing port** that is formatted like so: 
```
{
 type: pairing,
 noise: -64,
 confirmation: true
}
```
The `confirmation` field informs the root that data was received successfully and the leaf node is ready to begin communicating using the methods specified by the root. 

Upon receiving confirmation, the root will create a thread which creates the socket used to communicate with that specific leaf nonde. It will also create a shared memory buffer with the thread, allowing for data to be passed back and forth between the threads. This means that location data communicated by the leaf will be accessible by the root, and the root can place data that needs to be communicated to that child in the buffer. 

The root will repeat this pairing process until 3 leaves are paired. Thus, at the end of this stage, the root should have: **4 threads** (one for each of the leaf nodes and one for the root itself), **4 sockets** (one for each leaf node and one left over from the pairing phase), and **3 shared memory buffers** (one for each leaf to communicate data to and receive data from the root). Each of these threads is assigned an equal priority n where n <= 98 and set to the SCHED_RR scheduling algorithm, so that the root thread always runs before them and can send data to the children, or receive data from the children that was placed in their respective memory buffers during their last run.[^1]

##### Sniffer Establishment

In order to proceed with tracking devices using the signal strength, we must be able to quantifiably measure the RSSI. However, there isn't a reliable way to do this using sockets. In order to properly measure this, we choose to utilize the Radiotap header that is added when capturing packets in monitor mode. 

However, we can't sacrifice communication between the root and the leaf nodes, and placing a NIC in monitor mode causes the NIC to diassociate from the network. In order to maintain connections between the nodes, we have purchased three antennas with their own NIC's, each capable of monitor mode. Each antenna will be connected to a Raspberry Pi "leaf", which gives each Pi **two** NIC's instead of the standard one. This allows us to place one NIC in monitor mode (which allows us to continually monitor the network and capture all packets being transmitted near the Pi) and reserve the other NIC for communication between itself and the other leaves. 

For more details on how these capture sessions are created and facilitated, click [here](). 

##### Calibration Phase {#calibrate}

After pairing with the root, the distance of each leaf from each other must be determined in order to properly perform triangulation and accurately determine the location of both user and smart devices. However, pushing this responsibility onto the leaf nodes introduces both unnecessary levels of complexity and would likely result in more congestion on the network. Because of this, we choose to have the root node utilize the already existing communication channels that were established during the pairing process to coordinate each node and have all nodes receive the data needed. 

Upon entering this phase, the root will first send out a calibration packet as specified by the LML protocol. This packet is sent over the pairing socket (ensuring that every child is able to see it) and contains both the amount of packets to send over the pairing socket and the IPv6 address of the leaf node we want to communicate first. **Important note:** while the packet contains the IPv6 address of the leaf node, the packet is addressed to every node. This means that the packet uses the link-local address of IPv6 (ff02:1) and the pairing port to ensure every node receives the message and knows who is going to send their location data first.
#### {#link_to_calibration_packets}
```
{
    type: calibration, 
    num_of_calibration_packets: 100,
    leaf: fe80::603e:5fff:fe62:1556
}
```
<sub>An example of a LML calibrate packet from a root node to a leaf node.</sub>

Upon receiving the calibration packet, a leaf node waits for a total of 10 seconds before sending out response calibration packets. This allows it's "siblings" to save the IPv6 address of that node into their local database, and prepare to measure the distance between them and their sibling. 

After the 10 seconds expires, the leaf node sends out *x* calibration packets, where *x* is the number that was given by the root node. These packets are sent every *n* milliseconds, as specified during the pairing phase. These packets are addressed to the link-local address of IPv6 and pairing port, and contain very simple information so as to avoid constructing and deconstructing a complicated packet at each node:
```
{
    type: calibration,
    noise: -93,
    packets_remaining: 87,
}
```
<sub>An example of a calibration packet sent from a leaf node to a root. Note the `packets_remaining` field, which tells us how many calibration packets the leaf has yet to send. </sub>

Because of the address and port used to address these packets, they will be seen by the other leaves. However, instead of having these leafs receive these messages on their IPv6 socket that is listening on the pairing port, the leaves use the capture handles they have opened through the [pcap library](https://www.tcpdump.org/manpages/pcap.3pcap.html) to "sniff" the packets as they are sent from their siblings. As these packets are intercepted, the Radiotap header added on the packet when received is dissected and the RSSI (or signal strength) is extracted from the packet. However, because of the ever-changing landscape that is network conditions, the leaf will not inform the root that it has successfully received the RSSI from it's sibling until the number of remaining packets is 0. This allows us to average the RSSI values, so as to use a more accurate distance between the leaf and it's siblings. 

Once the `packets_remaining` field has dropped to 0, the sibling will then send a confirmation to the root indicating that it has successfully received the distance. Should a sibling not have successfully received any of the packets, it will set the confirmation to false so that the root can ask the leaf that was sending out the packets to resend it's data. An example of a leaf indicating successful estimation of the distance betweeen itself and it's sibling is below:

```
{
    type: calibration,
    noise: -87,
    leaf:  fe80::603e:5fff:fe62:1556, 
    confirmation: true
}

```
<sub>An example of a confirmation calibration packet sent from a leaf node to a root. Note the `leaf` field, which has the IPv6 address of the sibling the leaf was listening for, **not** the IPv6 address of the leaf sending the confirmation packet.</sub>

After receiving a confirmation from each of the leaves, the root then sends out a new calibration packet, just like [before](#link_to_calibration_packets), except with a different IPv6 address, which indicates a different leaf that needs to send data so it's siblings can register it's location. This process repeats for each leaf until each leaf has an accurate distance between it and it's siblings. This data is then passed along to the root at **the end of the calibration phase**, which allows the root to have proper measurememts for the device location phase. 

- [ ] : Add packet for sending distances here

##### Smart Device Discovery Phase

- This may need to be skipped/hardcoded at first.

##### User Tracking and Automation Phase

Once all smart devices have been registered to the system, the system enters it's primary phase, which involves both **tracking the user** and **responding to movement**. 

**Note: this should be under explanation for leaf node, but I haven't created that yet.**

Here, we've chosen to break down the order of explanation as based upon a "packet flow" - in other words, this part of the documentation will diagram the system's response when it captures a packet using it's packet handler.

First, the system will capture a packet that comes over the network. Upon receiving the packet, the system will parse the packet and look for it's Radiotap header. Once the Radiotap header is located, the RSSI is extracted and saved to a variable. Then, the system looks for the MAC address of the device who sent the packet. The MAC address, once found, is first compared to a list of blocked devices. This device list, which is determined by the root and communicated from the root to the leaves, represents devices which the system has already determined to be *ineligible for user tracking*. This means that after a candidate period of 24 hours, the system did not detect sufficient movement or variation of location of the device, which means that it likely is a stationary device - not a device the user often uses and carries with them. 

Assuming that the MAC address is not in our block list, the system then looks for the address in our *candidate list*. Devices in this list belong to one of two categories: **candidate** or **permanent**. Permanent devices represent devices that have been tracked by the system for a period *t* (where *t* >= 24 hours) and have been determined to be indicative of user activity. Should the device be present in our candidate list, the distance of the device from the node is immediately calculated, and that distance data (along with the MAC address) is passed to the socket for communication with the parent. 

If the device is *not* present in the candidate list, then the system looks for one final piece of information: the OUI[^2]. If OUI is not located in the packet, the packet is dropped and the system begins looking for other packets. If it is, the OUI is compared to a list of OUI's which represent the OUI's associated with companies that primarily manufacture user devices. If the OUI matches any of these companies, the device's distance from the leaf is calculated and the address and distance are passed to the socket to be communicated with the root; this also adds the device to our candidate list. 

[Continue](./System_Overview_cont.md)


[^1]: Documentation for the functions used to control CPU scheduling can be found [here](https://man7.org/linux/man-pages/man7/sched.7.html).

[^2]: For more information on OUI, please visit [this link](https://standards.ieee.org/faqs/regauth/) and look under the "What is a Organizationally Unique Identifier (OUI)?".